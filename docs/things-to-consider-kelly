Citation for OpenSubtitles2018:
	- website says, 
		- P. Lison and J. Tiedemann, 2016, OpenSubtitles2016: 
			Extracting Large Parallel Corpora from Movie and TV 
			Subtitles. In Proceedings of the 10th International 
			Conference on Language Resources and Evaluation (LREC 
			2016)
		- http://opus.nlpl.eu/OpenSubtitles2018.php
	- should prob actually be:
		http://www.lrec-conf.org/proceedings/lrec2018/pdf/294.pdf


I did the data processing in ~/complexity-transl
	- look at docs/workdoc-kelly, scripts/preprocess-data.sh, etc.

Philipp says our paper would be stronger if we:
	(1) Use paracrawl and split according to readability level, since using
	    different data could just be a domain effect.
	(2) Use other available scorers in addition to our two.

The learning rate hits 0.00 during training - does it do that too early, 
	preventing more gains?

If double decoder doesn't work, could be that it just needed to be optimised
	for the task (aka, maybe decoders need smaller params b/c they get 
	less data). Mention in MT write-up.

Our paper is both about readability *and* formality. Maybe make a second paper 
about formality?
	Maybe Paracrawl part could be more about readability, and Euro vs. OS
	about formality?

Maybe craig's is better at maintaining high bleu score whereas mine is better 
	at widening the gap.  I think craig's is better in lower resource envs
	b/c the decoder learns from all sentences (like corpus) -- mine may 
	suffers on corpus b/c each decoder gets limited examples (half of craig's)
Future work - craig used all examples - tagged middle ones neutrally so could 
	learn from them anyway. I eliminated those. I could try adding them and 
	alternating between decoders so I can take advantage of the extra data. 
	Maybe that will help me recover BLEU score since my BLEUs were lower 
	than his. 
