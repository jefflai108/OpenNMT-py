18 Nov
forked this repo
wrote and ran scripts/preprocess_data.sh
Ran quickstart code with:
	- python train.py -data data/demo -save_model demo-model -world_size 1 -gpu_ranks `free-gpu`

19 Nov
added checkpoint/ model/ out/ and qsub/ directories, with .gitignore in each
	- Thanks to https://stackoverflow.com/questions/115983/how-can-i-add-an-empty-directory-to-a-git-repository/932982#932982
	for the .gitignore trick.
copied/amended local-settings.sh and qsub_train.sh and their uses from Huda's 
	/home/huda/experiment/wmt18-filtering-de-en/nmt/gacha_10
copied/amended validate.{en/de}.sh from marian wmt2017-transformer example and 
	put in scripts
created train.sh and ran baseline experiments 0-2
added qsub-trans.sh and translate.sh

21 Nov
copied over issues-kelly, things-to-consider-kelly, and workdoc-kelly from 
	~/complexity-transl/docs to docs/. Renamed workdoc-kelly from that
	directory to workdoc-kelly-from-complexity-transl
copied get_readability.py from ~/complexity_transl to here.
updated translate.sh to call readability scorers
updated get_readability.py to handle summary score for entire file.
installed (outside VM? it seems to work OK) textstat with:
	pip install textstat
finished writing get_readability.py
wrote finished writing translate.sh and ran scoring tests 0-8

28 Nov
began editing code to handle 2 decoders
wrote preprocess_dd.py to preprocess data that will be handed to double decoders.
ran preprocessing of Euro+OS data for separate decoders using
	scripts/preprocess_data.sh
edited onmt/ for double decoder
edited train.sh to pass a var for double decoder
ran train.sh experiments 3-4

29 Nov
ran translate.sh experiments 9-14
copied corpus.bpe.100.shuf.{en/es} and europarl-v7.es-en.bpe.100.shuf.{en/es} 
	from Craig's /export/c06/cguo/data2 to data
copied paracrawl data to data/ with:
	wget https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-es.zipporah0-dedup-clean.tgz
copied preprocessing scripts from ~/complexity-transl/scripts to scripts/
renamed scripts/preprocess_data.sh to scripts/preprocess-data-onmt.sh 
renamed scripts/preprocess-data2.sh to scripts/preprocess-data-cleanjoin.sh 
copied ~/complexity_transl/model to this repository (contains 
	previously-trained BPE model and truecaser model.)
wrote and ran scripts/preprocess-paracrawl.sh
ran updated scripts/preprocess-data-onmt.sh for shuffled corpus & europarl data

30 Nov
renamed all shuf & paracrawl files in data/ that had .train. to .train0. 
	because that's what the training script expects now (after adding
	double decoder code)
added preprocessing for regular paracrawl data to scripts/preprocess-data-onmt.sh
	and ran it.
ran train.sh for trials 5-8  
